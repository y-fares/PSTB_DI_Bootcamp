# LLM configuration
LLM_BACKEND=groq
GROQ_API_KEY=YOUR_GROQ_KEY_HERE
GROQ_BASE_URL=https://api.groq.com/openai/v1
LLM_MODEL=llama-3.3-70b-versatile

# External MCP server 1: filesystem
MCP_FILES_CMD=npx
MCP_FILES_ARGS=@modelcontextprotocol/server-filesystem /home/yacine/mcp_root

# External MCP server 2: web/search
MCP_WEB_CMD=npx
MCP_WEB_ARGS=@modelcontextprotocol/server-web

# Your custom MCP server: local_insights (Part 2 requirement)
MCP_LOCAL_CMD=python
MCP_LOCAL_ARGS=my_mcp_server.py
