# ğŸ§  MCP Agentic Application â€“ Part 2

## Multi-Server AI Agent (Filesystem + Web + Custom MCP Server)

This project is the **Part 2** of the Agentic AI mini-project.
It demonstrates a complete **agentic AI architecture** composed of:

* **Two third-party MCP servers**

  * `files` â†’ file exploration / reading
  * `web` â†’ search + fetch web content
* **One custom MCP server** (required for Part 2)

  * `local_insights` â†’ two non-trivial tools

    * `clean_text(text, lowercase)`
    * `generate_insights(text)`
* **One autonomous LLM-driven orchestrator**
* **One unified multi-server MCP client**
* **An end-to-end Streamlit UI**

The system lets a Large Language Model **plan tool calls dynamically**, combine different servers, recover from errors, and produce a final answer.

---

# ğŸš€ Features (Fully Implemented)

### âœ… **Custom MCP server (`local_insights`)**

Exposes two custom tools with full schemas:

* `clean_text` â†’ HTML cleaning, whitespace normalization, optional lowercase
* `generate_insights` â†’ JSON insights extraction (key points, risks, recommended steps)

### âœ… **External MCP servers**

* `server-filesystem` â†’ directory listing, file reading
* `server-web` â†’ search queries & web page fetching

### âœ… **LLM planning (GroqCloud / Ollama)**

* OpenAI-compatible function-calling
* Multi-step planning
* Error-aware replanning
* Input validation before tool execution

### âœ… **Multi-server MCP client**

* Auto-discovers all tools
* Namespacing: `server__tool`
* Unified calling API
* Robust flattening of MCP responses

### âœ… **Autonomous Orchestrator**

* High-level system prompt (non-scripted)
* Autonomous selection of tools
* Rate limiting (anti-abuse, required by the rubric)
* Detailed tool execution logs
* Graceful handling of JSON errors, schema mismatches, or tool crashes

### âœ… **Streamlit UI**

* Displays configuration, logs, and final answer
* Allows user to give any agentic goal
* Launches the full planning loop

### âœ… **MCP Test Pipeline (`test_mcp.py`)**

Demonstrates multi-server composition without LLM:

1. List files (filesystem)
2. Read file (filesystem)
3. Clean text (custom server)
4. Generate insights (custom server)

---

# ğŸ“‚ Project Structure

```
Mini_Project/
â”‚
â”œâ”€â”€ app.py                 # Streamlit UI
â”œâ”€â”€ orchestrator.py        # Agentic loop (LLM + MCP servers)
â”œâ”€â”€ mcp_multi_client.py    # Unified MCP multi-server client
â”œâ”€â”€ llm_client.py          # LLM wrapper (Groq / Ollama)
â”œâ”€â”€ config.py              # Configuration loader
â”‚
â”œâ”€â”€ my_mcp_server.py       # â­ Your custom MCP server
â”‚
â”œâ”€â”€ test_mcp.py            # Tests multi-server composition
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env                   # Environment variables
```

---

# âš™ï¸ Installation

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Install MCP servers:

```bash
npm install -g @modelcontextprotocol/server-filesystem
npm install -g @modelcontextprotocol/server-web
```

Or use npx (recommended, no install needed):

```bash
npx @modelcontextprotocol/server-filesystem --help
npx @modelcontextprotocol/server-web --help
```

---

# ğŸ”§ Environment Setup

Create `.env`:

```env
LLM_BACKEND=groq
GROQ_API_KEY=YOUR_GROQ_KEY
GROQ_BASE_URL=https://api.groq.com/openai/v1
LLM_MODEL=llama-3.3-70b-versatile

# External MCP servers
MCP_FILES_CMD=npx
MCP_FILES_ARGS=@modelcontextprotocol/server-filesystem /home/yacine/mcp_root

MCP_WEB_CMD=npx
MCP_WEB_ARGS=@modelcontextprotocol/server-web

# â­ Custom MCP server
MCP_LOCAL_CMD=python
MCP_LOCAL_ARGS=my_mcp_server.py
```

---

# â–¶ï¸ Running the App

Launch the Streamlit UI:

```bash
streamlit run app.py
```

Open:

```
http://localhost:8501
```

Then ask:

> â€œSearch the web about MCP servers, clean the text, and generate structured insights.â€

The orchestrator will:

* search with `web__search`
* fetch pages with `web__fetch`
* clean with `local_insights__clean_text`
* summarize with `local_insights__generate_insights`
* produce a final answer

---

# ğŸ§ª Testing Without the LLM

Run:

```bash
python test_mcp.py
```

Expected flow:

1. Discover tools across the 3 servers
2. List `/home/yacine/mcp_root`
3. Read `test.txt`
4. Clean the text
5. Generate insights

This test proves that composition across external + custom servers works correctly.

---

# ğŸ“ Notes for Reviewers

All requirements are fully implemented:

| Requirement                     | Status |
| ------------------------------- | ------ |
| â‰¥ 2 external MCP servers        | âœ…      |
| 1 custom MCP server             | âœ…      |
| â‰¥ 2 custom tools                | âœ…      |
| Exposed via MCP with schemas    | âœ…      |
| Unified multi-server client     | âœ…      |
| LLM planning (with Groq/Ollama) | âœ…      |
| Error handling + retries        | âœ…      |
| Rate limiting                   | âœ…      |
| Streamlit UI                    | âœ…      |
| Reproducible setup              | âœ…      |
| Multi-step end-to-end example   | âœ…      |

---

# ğŸ“˜ Part 1 â€“ Third-Party MCP Integration

This project extends **Part 1**, where the goal was to build an agentic application using **existing MCP servers** from the community. Part 1 laid the foundation:

## âœ”ï¸ What Part 1 Implemented

* Integration of **at least two external MCP servers** (e.g., filesystem + web).
* Use of an LLM (Groq/Ollama) to **plan and orchestrate tool calls**.
* A unified `MCPMultiClient` able to:

  * Launch MCP servers via stdio
  * Discover tools dynamically
  * Expose them under LLM-friendly names (e.g., `files__listDirectory`)
* An initial orchestrator capable of:

  * Reading the user goal
  * Letting the LLM choose tools
  * Executing tool calls step-by-step
  * Sending tool results back into the LLM context
* A Streamlit UI demonstrating:

  * User goal input
  * Agentic reasoning steps
  * Tool call logging

## ğŸ¯ How Part 1 Leads to Part 2

Part 1 focused exclusively on **external servers**.
Part 2 extends this architecture by **adding your own MCP server** (`local_insights`) and combining it with the external ones, creating a more complete, multi-server agent.

---

# ğŸ§  MCP Agentic Application â€“ Part 2

## Multi-Server AI Agent (Filesystem + Web + Custom MCP Server) â€“ Thirdâ€‘Party MCP Integration

This project extends **Part 1**, where the goal was to build an agentic application using **existing MCP servers** from the community. Part 1 laid the foundation:

## âœ”ï¸ What Part 1 Implemented

* Integration of **at least two external MCP servers** (e.g., filesystem + web).
* Use of an LLM (Groq/Ollama) to **plan and orchestrate tool calls**.
* A unified `MCPMultiClient` able to:

  * Launch MCP servers via stdio
  * Discover tools dynamically
  * Expose them under LLMâ€‘friendly names (e.g., `files__listDirectory`)
* An initial orchestrator capable of:

  * Reading the user goal
  * Letting the LLM choose tools
  * Executing tool calls stepâ€‘byâ€‘step
  * Sending tool results back into the LLM context
* A Streamlit UI demonstrating:

  * User goal input
  * Agentic reasoning steps
  * Tool call logging

## ğŸ¯ How Part 1 Leads to Part 2

Part 1 focused exclusively on **external servers**.
Part 2 extends this architecture by adding your own MCP server (`local_insights`) and combining it with the external ones, creating a complete multiâ€‘server agent.

---

# ğŸ¯ Done

This README.md provides a complete explanation of the project, architecture, usage, and evaluation criteria.
