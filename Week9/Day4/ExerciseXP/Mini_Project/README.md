# ğŸ§  MCP Agentic Application â€“ Part 2  
## Multi-Server AI Agent (Filesystem + Web + Custom MCP Server)

This project is the **Part 2** of the Agentic AI mini-project.  
It demonstrates a complete **agentic AI architecture** composed of:

- **Two third-party MCP servers**  
  - `files` â†’ file exploration / reading  
  - `web` â†’ search + fetch web content  
- **One custom MCP server** (required for Part 2)  
  - `local_insights` â†’ two non-trivial tools  
    - `clean_text(text, lowercase)`  
    - `generate_insights(text)`  
- **One autonomous LLM-driven orchestrator**  
- **One unified multi-server MCP client**  
- **An end-to-end Streamlit UI**

The system lets a Large Language Model **plan tool calls dynamically**, combine different servers, recover from errors, and produce a final answer.

---

# ğŸš€ Features (Fully Implemented)

### âœ… **Custom MCP server (`local_insights`)**
Exposes two custom tools with full schemas:
- `clean_text` â†’ HTML cleaning, whitespace normalization, optional lowercase  
- `generate_insights` â†’ JSON insights extraction (key points, risks, recommended steps)

### âœ… **External MCP servers**
- `server-filesystem` â†’ directory listing, file reading  
- `server-web` â†’ search queries & web page fetching  

### âœ… **LLM planning (GroqCloud / Ollama)**
- OpenAI-compatible function-calling  
- Multi-step planning  
- Error-aware replanning  
- Input validation before tool execution

### âœ… **Multi-server MCP client**
- Auto-discovers all tools  
- Namespacing: `server__tool`  
- Unified calling API  
- Robust flattening of MCP responses  

### âœ… **Autonomous Orchestrator**
- High-level system prompt (non-scripted)  
- Autonomous selection of tools  
- Rate limiting (anti-abuse, required by the rubric)  
- Detailed tool execution logs  
- Graceful handling of JSON errors, schema mismatches, or tool crashes  

### âœ… **Streamlit UI**
- Displays configuration, logs, and final answer  
- Allows user to give any agentic goal  
- Launches the full planning loop  

### âœ… **MCP Test Pipeline (`test_mcp.py`)**
Demonstrates multi-server composition without LLM:
1. List files (filesystem)  
2. Read file (filesystem)  
3. Clean text (custom server)  
4. Generate insights (custom server)  

---

# ğŸ“‚ Project Structure

Mini_Project/
â”‚
â”œâ”€â”€ app.py # Streamlit UI
â”œâ”€â”€ orchestrator.py # Agentic loop (LLM + MCP servers)
â”œâ”€â”€ mcp_multi_client.py # Unified MCP multi-server client
â”œâ”€â”€ llm_client.py # LLM wrapper (Groq / Ollama)
â”œâ”€â”€ config.py # Configuration loader
â”‚
â”œâ”€â”€ my_mcp_server.py # â­ Your custom MCP server
â”‚
â”œâ”€â”€ test_mcp.py # Tests multi-server composition
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env # Environment variables

yaml
Copier le code

---

# âš™ï¸ Installation

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
Install MCP servers:

bash
Copier le code
npm install -g @modelcontextprotocol/server-filesystem
npm install -g @modelcontextprotocol/server-web
Or use npx (recommended, no install needed):

bash
Copier le code
npx @modelcontextprotocol/server-filesystem --help
npx @modelcontextprotocol/server-web --help
ğŸ”§ Environment Setup
Create .env:

env
Copier le code
LLM_BACKEND=groq
GROQ_API_KEY=YOUR_GROQ_KEY
GROQ_BASE_URL=https://api.groq.com/openai/v1
LLM_MODEL=llama-3.3-70b-versatile

# External MCP servers
MCP_FILES_CMD=npx
MCP_FILES_ARGS=@modelcontextprotocol/server-filesystem /home/yacine/mcp_root

MCP_WEB_CMD=npx
MCP_WEB_ARGS=@modelcontextprotocol/server-web

# â­ Custom MCP server
MCP_LOCAL_CMD=python
MCP_LOCAL_ARGS=my_mcp_server.py
â–¶ï¸ Running the App
Launch the Streamlit UI:

bash
Copier le code
streamlit run app.py
Open:

arduino
Copier le code
http://localhost:8501
Then ask:

â€œSearch the web about MCP servers, clean the text, and generate structured insights.â€

The orchestrator will:

search with web__search

fetch pages with web__fetch

clean with local_insights__clean_text

summarize with local_insights__generate_insights

produce a final answer

ğŸ§ª Testing Without the LLM
Run:

bash
Copier le code
python test_mcp.py
Expected flow:

Discover tools across the 3 servers

List /home/yacine/mcp_root

Read test.txt

Clean the text

Generate insights

This test proves that composition across external + custom servers works correctly.